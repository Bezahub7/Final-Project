[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST-558 Final Project",
    "section": "",
    "text": "#Load libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(skimr)\nlibrary(purrr)\nlibrary(gt)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(tidymodels)\nlibrary(doParallel)"
  },
  {
    "objectID": "EDA.html#task-1read-in-the-dataset",
    "href": "EDA.html#task-1read-in-the-dataset",
    "title": "ST-558 Final Project",
    "section": "Task 1:Read in the dataset",
    "text": "Task 1:Read in the dataset\n\n#Read in the dataset\n\ndiabetes &lt;- read.csv(\"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\nhead(diabetes)\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      1        1         1  40      1      0\n2               0      0        0         0  25      1      0\n3               0      1        1         1  28      0      0\n4               0      1        0         1  27      0      0\n5               0      1        1         1  24      0      0\n6               0      1        1         1  25      1      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       1                 0\n2                    0            1      0       0                 0\n3                    0            0      1       0                 0\n4                    0            1      1       1                 0\n5                    0            1      1       1                 0\n6                    0            1      1       1                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             1           0       5       18       15        1   0   9\n2             0           1       3        0        0        0   0   7\n3             1           1       5       30       30        1   0   9\n4             1           0       2        0        0        0   0  11\n5             1           0       2        3        0        0   0  11\n6             1           0       2        0        2        0   1  10\n  Education Income\n1         4      3\n2         6      1\n3         4      8\n4         3      6\n5         5      4\n6         6      8"
  },
  {
    "objectID": "EDA.html#task-2data-cleaning-and-preparation",
    "href": "EDA.html#task-2data-cleaning-and-preparation",
    "title": "ST-558 Final Project",
    "section": "Task 2:Data Cleaning and Preparation",
    "text": "Task 2:Data Cleaning and Preparation\n\n# convert integer variables into factors\n# define the labels for those with large number of distinct levels as a character vector\nage_labels &lt;- c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\",\n                  \"55-59\", \"60-64\", \"65-69\", \"70-74\", \"75-79\",\"80-99\")\nincome_labels &lt;- c(\"&lt;$10,000\", \"&lt;$15,000\", \"&lt;$20,000\", \"&lt;$25,000\", \"&lt;$35,000\",                        \"&lt;$50,000\", \"&lt;$75,000\",\"&gt;=$75,000\")\n  \n\ndiabetes1 &lt;- diabetes |&gt;\n  #add label for all variables with the same label at once\n  mutate(across(c(Smoker,Stroke,HeartDiseaseorAttack,PhysActivity,Fruits,Veggies\n                  ,HvyAlcoholConsump,AnyHealthcare,NoDocbcCost,DiffWalk),\n                ~factor(.x,levels = c(0,1),\n                        labels = c(\"No\",\"Yes\"))))|&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary,\n          levels = c(0,1), labels = c(\"No diabetes\", \"Diabetes\")),\n    HighBP = factor(HighBP,\n          levels = c(0,1), labels = c(\"No high BP\", \"High BP\")),\n    HighChol = factor(HighChol,\n          levels = c(0,1), labels = c(\"No high cholestrol\", \"High cholestrol\")),\n    CholCheck = factor(CholCheck,\n          levels = c(0,1), labels = c(\"No cholestrol Check in 5 yrs\",                                                     \"Cholestrol checked in 5 yrs\")),\n    GenHlth = factor(GenHlth,\n          levels = c(1,2,3,4,5), \n          labels = c(\"Excellent\",\"Very good\",\"Good\",\"Fair\",\"Poor\")),\n    Sex = factor(Sex,\n          levels = c(0,1), labels = c(\"Female\",\"Male\")),\n    Education = factor(Education,\n      levels = c(1,2,3,4,5,6), labels = c(\"Unschooled\",\n                                          \"Grades 1-8\",\n                                          \"Grades 9-11\",\n                                          \"Grade 12 or GED\",        \n                                          \"Some College\",\n                                          \"College Grad\")),\n    \n  )|&gt;\n  mutate (Age_group=factor(Age,labels = age_labels),\n          .keep=\"unused\")|&gt;\n\n  mutate(Income_group=factor(Income,labels = income_labels),\n         .keep=\"unused\")\n\nhead(diabetes1)\n\n  Diabetes_binary     HighBP           HighChol                    CholCheck\n1     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n2     No diabetes No high BP No high cholestrol No cholestrol Check in 5 yrs\n3     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n4     No diabetes    High BP No high cholestrol  Cholestrol checked in 5 yrs\n5     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n6     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n  BMI Smoker Stroke HeartDiseaseorAttack PhysActivity Fruits Veggies\n1  40    Yes     No                   No           No     No     Yes\n2  25    Yes     No                   No          Yes     No      No\n3  28     No     No                   No           No    Yes      No\n4  27     No     No                   No          Yes    Yes     Yes\n5  24     No     No                   No          Yes    Yes     Yes\n6  25    Yes     No                   No          Yes    Yes     Yes\n  HvyAlcoholConsump AnyHealthcare NoDocbcCost   GenHlth MentHlth PhysHlth\n1                No           Yes          No      Poor       18       15\n2                No            No         Yes      Good        0        0\n3                No           Yes         Yes      Poor       30       30\n4                No           Yes          No Very good        0        0\n5                No           Yes          No Very good        3        0\n6                No           Yes          No Very good        0        2\n  DiffWalk    Sex       Education Age_group Income_group\n1      Yes Female Grade 12 or GED     60-64     &lt;$20,000\n2       No Female    College Grad     50-54     &lt;$10,000\n3      Yes Female Grade 12 or GED     60-64    &gt;=$75,000\n4       No Female     Grades 9-11     70-74     &lt;$50,000\n5       No Female    Some College     70-74     &lt;$25,000\n6       No   Male    College Grad     65-69    &gt;=$75,000\n\ncolnames(diabetes1)\n\n [1] \"Diabetes_binary\"      \"HighBP\"               \"HighChol\"            \n [4] \"CholCheck\"            \"BMI\"                  \"Smoker\"              \n [7] \"Stroke\"               \"HeartDiseaseorAttack\" \"PhysActivity\"        \n[10] \"Fruits\"               \"Veggies\"              \"HvyAlcoholConsump\"   \n[13] \"AnyHealthcare\"        \"NoDocbcCost\"          \"GenHlth\"             \n[16] \"MentHlth\"             \"PhysHlth\"             \"DiffWalk\"            \n[19] \"Sex\"                  \"Education\"            \"Age_group\"           \n[22] \"Income_group\"        \n\n#save the data as.RDS file to access it in the modeling.qmd\n\ndiabetes_final &lt;- diabetes1\nsaveRDS(diabetes_final,\"data/diabetes_final.rds\")\n\n\n# Count total missing per variable\ndiabetes1 |&gt; dplyr::summarize(across(everything(), ~ sum(is.na(.))))\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      0        0         0   0      0      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       0                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Education\n1             0           0       0        0        0        0   0         0\n  Age_group Income_group\n1         0            0\n\n# Visual proportion\nmissing_summary &lt;- diabetes1 |&gt; \n  dplyr::summarize(across(everything(), ~ mean(is.na(.)))) |&gt; \n  pivot_longer(everything(), names_to = \"variable\", values_to = \"missing_prop\")\n\nggplot(missing_summary, aes(x = reorder(variable, -missing_prop), y = missing_prop)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Proportion of Missing Data by Variable\",\n       x = \"Variable\", y = \"Missing Proportion\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBefore performing any modeling, it is important to evaluate the extent of missingness in the dataset. Across the 22 variables, the summary shows that missingness is absent for all variables.\nThe bar plot of missing proportions confirms that no variable contains any missing data, meaning the dataset is generally complete and suitable for predictive modeling."
  },
  {
    "objectID": "EDA.html#task-3summary-statistics",
    "href": "EDA.html#task-3summary-statistics",
    "title": "ST-558 Final Project",
    "section": "Task 3:Summary Statistics",
    "text": "Task 3:Summary Statistics\n\nSummary for Factor Variables\n\nOne way contingency tables\nThe following code gives frequency plus percent of each factor.\n\nfactor_summary &lt;-diabetes1|&gt;\n  select(where(is.factor))|&gt;\n  pivot_longer(everything(),names_to = \"Variable\", values_to = \"Value\")|&gt;\n  group_by(Variable,Value)|&gt;\n  summarise(Count=n(),.groups = \"drop\")|&gt;\n  group_by(Variable)|&gt;\n  mutate(Percent=round(100*Count/sum(Count),1))|&gt;\n  ungroup()\nfactor_summary\n\n# A tibble: 62 × 4\n   Variable  Value Count Percent\n   &lt;chr&gt;     &lt;fct&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Age_group 18-24  5700     2.2\n 2 Age_group 25-29  7598     3  \n 3 Age_group 30-34 11123     4.4\n 4 Age_group 35-39 13823     5.4\n 5 Age_group 40-44 16157     6.4\n 6 Age_group 45-49 19819     7.8\n 7 Age_group 50-54 26314    10.4\n 8 Age_group 55-59 30832    12.2\n 9 Age_group 60-64 33244    13.1\n10 Age_group 65-69 32194    12.7\n# ℹ 52 more rows\n\n\n\n\nTwo way contingency tables\n\n#with proportion\n\nprop.table(table(diabetes1$Diabetes_binary,diabetes1$PhysActivity))\n\n             \n                      No        Yes\n  No diabetes 0.19197808 0.66868890\n  Diabetes    0.05147824 0.08785478\n\n\nFrom the table we can see that 26604 participant have both Diabetes and High BP. And 66.8% of the participants who exercise don’t have Diabetes.\n\n\nThree way contingency tables\n\ntable(diabetes1$Diabetes_binary,diabetes1$HighBP,diabetes1$Smoker)\n\n, ,  = No\n\n             \n              No high BP High BP\n  No diabetes      82337   41891\n  Diabetes          4370   12659\n\n, ,  = Yes\n\n             \n              No high BP High BP\n  No diabetes      53772   40334\n  Diabetes          4372   13945\n\n\nout of 141,257 non smokers, 12659(~9% ) have diabetes and high BP out of 112,423 smokers, 13945(12.4%) have diabetes and high BP Even if the total number of smokers are relatively smaller, we have a higher percentage of people with both conditions (diabetes and high BP)\n\n\n\nSummary for Numeric Variables\nThe following code gives summary of the numeric varaibles. Numeric vars = BMI, MentHlth, PhysHlth\n\nnum_summry &lt;- diabetes1 |&gt; \n  select(BMI, MentHlth, PhysHlth) |&gt; \n  skim()\nnum_summry\n\n\nData summary\n\n\nName\nselect(diabetes1, BMI, Me…\n\n\nNumber of rows\n253680\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nBMI\n0\n1\n28.38\n6.61\n12\n24\n27\n31\n98\n▇▅▁▁▁\n\n\nMentHlth\n0\n1\n3.18\n7.41\n0\n0\n0\n2\n30\n▇▁▁▁▁\n\n\nPhysHlth\n0\n1\n4.24\n8.72\n0\n0\n0\n3\n30\n▇▁▁▁▁\n\n\n\n\n\n\n\nExplore the Outcome: Diabetes_binary\n\n#table\ndiabetes1 |&gt; \n  tabyl(Diabetes_binary) |&gt; \n  adorn_pct_formatting()\n\n Diabetes_binary      n percent\n     No diabetes 218334   86.1%\n        Diabetes  35346   13.9%\n\n\n\n#plot\nggplot(diabetes1, aes(x = Diabetes_binary)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Distribution of Diabetes Outcome\", \n       x = \"Diabetes Status\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe outcome variable Diabetes_binary is imbalanced, with the majority of respondents classified as non-diabetic (0). This imbalance is important to note, because predictive modeling methods may require tuning or evaluation metrics that account for imbalance (e.g., ROC-AUC rather than accuracy alone).\n\n\nBivariate Exploration: Predictors vs. Diabetes_binary\nLet’s explore how predictors relate to Diabetes_binary.\n\nDiabetes_binary vs predictors\n\nContigency table: Diabetes_binary vs HighBP\n\n#code\ndiabetes1 |&gt; \n  dplyr::count(Diabetes_binary,HighBP) |&gt; \n  group_by(HighBP) |&gt; \n  dplyr::mutate(prop = n/sum(n))\n\n# A tibble: 4 × 4\n# Groups:   HighBP [2]\n  Diabetes_binary HighBP          n   prop\n  &lt;fct&gt;           &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;\n1 No diabetes     No high BP 136109 0.940 \n2 No diabetes     High BP     82225 0.756 \n3 Diabetes        No high BP   8742 0.0604\n4 Diabetes        High BP     26604 0.244 \n\n\n\n\nPlot: Diabetes_binary vs HighBP\nvisuals for Diabetes_binary vs some selected predictor variables.\n\npredictors &lt;- c(\"HighBP\", \"Fruits\", \"Veggies\", \"PhysActivity\", \"Smoker\", \"HighChol\", \"Income_group\", \"Education\", \"Age_group\")\n\nfor (var in predictors) {\n  p &lt;- ggplot(diabetes1, aes_string(x = var, fill = \"Diabetes_binary\")) +\n    geom_bar(position = \"fill\") +\n    labs(\n      title = paste(\"Proportion of Diabetes by\", var),\n      x = var,\n      y = \"Proportion\"\n    ) +\n    scale_y_continuous(labels = scales::percent) +\n    theme_minimal()\n  \n  print(p)   \n}\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bivariate visualizations were created to examine how each predictor is associated with out come variable (diabetes). Several variables show strong separation. Individuals with a history of high blood pressure or high cholesterol have a higher proportion of diabetes. Lifestyle factors, such as physical activity and poor diet (low fruit/vegetable intake), also show noticeable differences in diabetes prevalence. For Socio-demographic variables such as age, income, and education we see that older adults and individuals with lower income or education levels tend to have higher diabetes rates.\n\n\n\n\nNumeric predictors vs. Diabetes_binary\n\nPlot: BMI, Mental Health and Physical Health\n\nvars &lt;- c(\"BMI\", \"MentHlth\", \"PhysHlth\")\nvar_labels &lt;- c(\"BMI\", \"Mental Health\", \"Physical Health\")\n\nfor (i in seq_along(vars)) {\n  \n  v     &lt;- vars[i]      # variable name\n  label &lt;- var_labels[i]    # label for the y-axis/title\n  \n  p &lt;- ggplot(diabetes1,\n              aes(x = .data[[\"Diabetes_binary\"]],\n                  y = .data[[v]])) +\n    geom_boxplot(fill = \"tan\") +\n    labs(\n      title = paste(label, \"Distribution by Diabetes Status\"),\n      x = \"Diabetes Status\",\n      y = label\n    ) +\n    theme_minimal()\n  \n  print(p)   # show plot\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI differs markedly between individuals with and without diabetes, with diabetics showing higher median BMI. This aligns with known clinical risk patterns. Mental and physical health days show weaker or more variable separation, though diabetics tend to report more physically unhealthy days.\n\n\n\nCorrelations Among Numeric Variables\n\ndiabetes1 |&gt; \n  select(BMI, MentHlth, PhysHlth) |&gt; \n  cor(use = \"pairwise.complete.obs\")\n\n                BMI   MentHlth  PhysHlth\nBMI      1.00000000 0.08531016 0.1211411\nMentHlth 0.08531016 1.00000000 0.3536189\nPhysHlth 0.12114111 0.35361887 1.0000000\n\n\nAmong the numeric variables, mental health and physical health are some how correlated.\nOverall, the exploratory data analysis shows several strong relationships that are relevant for predictive modeling. Cardiometabolic variables such as high blood pressure and high cholesterol show clear associations with diabetes status. Lifestyle factors including physical activity and diet also demonstrate meaningful differences between diabetic and non-diabetic groups. Socio-demographic variables such as age, income, and education—indicate disparities that may inform model performance. The next step is to formally evaluate multiple models, compare their predictive performance, and select a final model to be deployed as an API."
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Predictive Modeling",
    "section": "",
    "text": "#Load libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(skimr)\nlibrary(purrr)\nlibrary(gt)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(reshape2)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(tidymodels)\nlibrary(doParallel)\nregisterDoParallel()"
  },
  {
    "objectID": "EDA.html#click-here-for-the-modeling-page",
    "href": "EDA.html#click-here-for-the-modeling-page",
    "title": "ST-558 Final Project",
    "section": "Click here for the Modeling Page",
    "text": "Click here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html#task1-read-data",
    "href": "Modeling.html#task1-read-data",
    "title": "Predictive Modeling",
    "section": "Task1: Read data",
    "text": "Task1: Read data\nThe cleaned data was already saved in the data folder of this project so, I’ll read it in.\n\ndiabetes_final&lt;-readRDS(\"diabetes_final.rds\")"
  },
  {
    "objectID": "Modeling.html#task2-split-data",
    "href": "Modeling.html#task2-split-data",
    "title": "Predictive Modeling",
    "section": "Task2: Split data",
    "text": "Task2: Split data\nSplit the data into a training (70% of the data) and test set (30% of the data). Set a seed to make things reproducible.\n\n#Stratified sampling (This is used because the outcome is an imbalance outcome)\nset.seed(123)\n\nsplit &lt;- initial_split(diabetes_final, prop = 0.7, strata = Diabetes_binary)\n\ntrain_data &lt;- training(split)\nhead(train_data)\n\n  Diabetes_binary     HighBP           HighChol                   CholCheck BMI\n1        Diabetes    High BP    High cholestrol Cholestrol checked in 5 yrs  30\n2        Diabetes No high BP No high cholestrol Cholestrol checked in 5 yrs  25\n3        Diabetes    High BP    High cholestrol Cholestrol checked in 5 yrs  28\n4        Diabetes No high BP No high cholestrol Cholestrol checked in 5 yrs  23\n5        Diabetes    High BP No high cholestrol Cholestrol checked in 5 yrs  27\n6        Diabetes    High BP    High cholestrol Cholestrol checked in 5 yrs  28\n  Smoker Stroke HeartDiseaseorAttack PhysActivity Fruits Veggies\n1    Yes     No                  Yes           No    Yes     Yes\n2    Yes     No                   No          Yes    Yes     Yes\n3     No     No                   No           No     No     Yes\n4    Yes     No                   No          Yes     No      No\n5     No     No                   No          Yes    Yes     Yes\n6    Yes     No                  Yes           No     No     Yes\n  HvyAlcoholConsump AnyHealthcare NoDocbcCost   GenHlth MentHlth PhysHlth\n1                No           Yes          No      Poor       30       30\n2                No           Yes          No      Good        0        0\n3                No           Yes          No      Fair        0        0\n4                No           Yes          No Very good        0        0\n5                No           Yes          No Excellent        0        0\n6                No           Yes          No      Fair        0        0\n  DiffWalk    Sex       Education Age_group Income_group\n1      Yes Female    Some College     60-64     &lt;$10,000\n2       No   Male    College Grad     80-99    &gt;=$75,000\n3      Yes Female Grade 12 or GED     70-74     &lt;$50,000\n4       No   Male    Some College     50-54     &lt;$50,000\n5       No Female    Some College     80-99     &lt;$25,000\n6       No   Male      Grades 1-8     75-79     &lt;$25,000\n\ntest_data  &lt;- testing(split)\nhead(test_data)\n\n  Diabetes_binary     HighBP           HighChol                    CholCheck\n1     No diabetes No high BP No high cholestrol No cholestrol Check in 5 yrs\n2     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n3     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n4     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n5     No diabetes    High BP    High cholestrol  Cholestrol checked in 5 yrs\n6     No diabetes No high BP No high cholestrol  Cholestrol checked in 5 yrs\n  BMI Smoker Stroke HeartDiseaseorAttack PhysActivity Fruits Veggies\n1  25    Yes     No                   No          Yes     No      No\n2  28     No     No                   No           No    Yes      No\n3  25    Yes     No                   No          Yes     No     Yes\n4  34    Yes     No                   No           No    Yes     Yes\n5  28    Yes     No                   No           No    Yes     Yes\n6  32     No     No                   No          Yes    Yes     Yes\n  HvyAlcoholConsump AnyHealthcare NoDocbcCost   GenHlth MentHlth PhysHlth\n1                No            No         Yes      Good        0        0\n2                No           Yes         Yes      Poor       30       30\n3                No           Yes          No      Good        0        0\n4                No           Yes          No      Good        0       30\n5                No           Yes          No      Good        6        0\n6                No           Yes          No Very good        0        0\n  DiffWalk    Sex       Education Age_group Income_group\n1       No Female    College Grad     50-54     &lt;$10,000\n2      Yes Female Grade 12 or GED     60-64    &gt;=$75,000\n3      Yes Female Grade 12 or GED     70-74     &lt;$25,000\n4      Yes Female    Some College     65-69     &lt;$10,000\n5      Yes Female Grade 12 or GED     60-64     &lt;$50,000\n6       No Female    College Grad     40-44    &gt;=$75,000"
  },
  {
    "objectID": "Modeling.html#task3-crross-validation-set-up-5-fold-stratified",
    "href": "Modeling.html#task3-crross-validation-set-up-5-fold-stratified",
    "title": "Predictive Modeling",
    "section": "Task3: Crross-validation set up (5-fold, stratified)",
    "text": "Task3: Crross-validation set up (5-fold, stratified)\n\nset.seed(124)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = Diabetes_binary)\ncv_folds\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [142059/35516]&gt; Fold1\n2 &lt;split [142059/35516]&gt; Fold2\n3 &lt;split [142060/35515]&gt; Fold3\n4 &lt;split [142061/35514]&gt; Fold4\n5 &lt;split [142061/35514]&gt; Fold5\n\n\n##Task 4 Recipe: Basic recipe using all predictors.\n\ndiabetes_recipe &lt;- recipe(Diabetes_binary ~ ., data = train_data) %&gt;%\n  step_dummy(all_nominal_predictors(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors())\ndiabetes_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 21\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors() -all_outcomes()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "Modeling.html#task4-classification-tree",
    "href": "Modeling.html#task4-classification-tree",
    "title": "Predictive Modeling",
    "section": "Task4: Classification Tree",
    "text": "Task4: Classification Tree\n\nModel specification (with parameters to tune)\nWe’ll tune tree depth and cost-complexity (cp).\n\ntree_model &lt;- decision_tree(\n  cost_complexity = tune(),\n  tree_depth = tune(),\n  min_n = 5   # fixed value but could also tune\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\ntree_model\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 5\n\nComputational engine: rpart \n\n\n\n\nGrid of tuning parameters ( custom grid)\n\ntree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-5, -1)),   # log10 scale → 1e-5 to 1e-1\n  tree_depth(range = c(2, 10)),\n  levels = 5\n)\ntree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1         0.00001          2\n 2         0.0001           2\n 3         0.001            2\n 4         0.01             2\n 5         0.1              2\n 6         0.00001          4\n 7         0.0001           4\n 8         0.001            4\n 9         0.01             4\n10         0.1              4\n# ℹ 15 more rows\n\n\n\n\nWorkflow for the tree\n\ntree_wf &lt;- workflow() %&gt;%\n  add_model(tree_model) %&gt;%\n  add_recipe(diabetes_recipe)\ntree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 5\n\nComputational engine: rpart \n\n\n\n\nTuning using LOG-LOSS\n\n# Set up parallel backend \ncl &lt;- makeCluster(parallel::detectCores() - 1)\nregisterDoParallel(cl)\n\n# Control settings for tuning\nctrl &lt;- control_grid(\n  save_pred     = FALSE,\n  save_workflow = FALSE,\n  verbose       = TRUE,\n  allow_par     = TRUE\n)\n\n# Run tuning\nset.seed(112)\ntree_res &lt;- tune_grid(\n  tree_wf,\n  resamples = cv_folds,\n  metrics   = metric_set(mn_log_loss),\n  grid      = tree_grid,   # &lt;- use the object, NOT tree_grid(...)\n  control   = ctrl\n)\n\ni Fold1: preprocessor 1/1\n\n\ni Fold1: preprocessor 1/1, model 1/25\n\n\ni Fold1: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 2/25\n\n\ni Fold1: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 3/25\n\n\ni Fold1: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 4/25\n\n\ni Fold1: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 5/25\n\n\ni Fold1: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 6/25\n\n\ni Fold1: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 7/25\n\n\ni Fold1: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 8/25\n\n\ni Fold1: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 9/25\n\n\ni Fold1: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 10/25\n\n\ni Fold1: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 11/25\n\n\ni Fold1: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 12/25\n\n\ni Fold1: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 13/25\n\n\ni Fold1: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 14/25\n\n\ni Fold1: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 15/25\n\n\ni Fold1: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 16/25\n\n\ni Fold1: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 17/25\n\n\ni Fold1: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 18/25\n\n\ni Fold1: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 19/25\n\n\ni Fold1: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 20/25\n\n\ni Fold1: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 21/25\n\n\ni Fold1: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 22/25\n\n\ni Fold1: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 23/25\n\n\ni Fold1: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 24/25\n\n\ni Fold1: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 25/25\n\n\ni Fold1: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold2: preprocessor 1/1\n\n\ni Fold2: preprocessor 1/1, model 1/25\n\n\ni Fold2: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 2/25\n\n\ni Fold2: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 3/25\n\n\ni Fold2: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 4/25\n\n\ni Fold2: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 5/25\n\n\ni Fold2: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 6/25\n\n\ni Fold2: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 7/25\n\n\ni Fold2: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 8/25\n\n\ni Fold2: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 9/25\n\n\ni Fold2: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 10/25\n\n\ni Fold2: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 11/25\n\n\ni Fold2: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 12/25\n\n\ni Fold2: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 13/25\n\n\ni Fold2: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 14/25\n\n\ni Fold2: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 15/25\n\n\ni Fold2: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 16/25\n\n\ni Fold2: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 17/25\n\n\ni Fold2: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 18/25\n\n\ni Fold2: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 19/25\n\n\ni Fold2: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 20/25\n\n\ni Fold2: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 21/25\n\n\ni Fold2: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 22/25\n\n\ni Fold2: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 23/25\n\n\ni Fold2: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 24/25\n\n\ni Fold2: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 25/25\n\n\ni Fold2: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold3: preprocessor 1/1\n\n\ni Fold3: preprocessor 1/1, model 1/25\n\n\ni Fold3: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 2/25\n\n\ni Fold3: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 3/25\n\n\ni Fold3: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 4/25\n\n\ni Fold3: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 5/25\n\n\ni Fold3: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 6/25\n\n\ni Fold3: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 7/25\n\n\ni Fold3: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 8/25\n\n\ni Fold3: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 9/25\n\n\ni Fold3: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 10/25\n\n\ni Fold3: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 11/25\n\n\ni Fold3: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 12/25\n\n\ni Fold3: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 13/25\n\n\ni Fold3: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 14/25\n\n\ni Fold3: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 15/25\n\n\ni Fold3: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 16/25\n\n\ni Fold3: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 17/25\n\n\ni Fold3: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 18/25\n\n\ni Fold3: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 19/25\n\n\ni Fold3: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 20/25\n\n\ni Fold3: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 21/25\n\n\ni Fold3: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 22/25\n\n\ni Fold3: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 23/25\n\n\ni Fold3: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 24/25\n\n\ni Fold3: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 25/25\n\n\ni Fold3: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold4: preprocessor 1/1\n\n\ni Fold4: preprocessor 1/1, model 1/25\n\n\ni Fold4: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 2/25\n\n\ni Fold4: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 3/25\n\n\ni Fold4: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 4/25\n\n\ni Fold4: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 5/25\n\n\ni Fold4: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 6/25\n\n\ni Fold4: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 7/25\n\n\ni Fold4: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 8/25\n\n\ni Fold4: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 9/25\n\n\ni Fold4: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 10/25\n\n\ni Fold4: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 11/25\n\n\ni Fold4: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 12/25\n\n\ni Fold4: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 13/25\n\n\ni Fold4: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 14/25\n\n\ni Fold4: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 15/25\n\n\ni Fold4: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 16/25\n\n\ni Fold4: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 17/25\n\n\ni Fold4: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 18/25\n\n\ni Fold4: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 19/25\n\n\ni Fold4: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 20/25\n\n\ni Fold4: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 21/25\n\n\ni Fold4: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 22/25\n\n\ni Fold4: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 23/25\n\n\ni Fold4: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 24/25\n\n\ni Fold4: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 25/25\n\n\ni Fold4: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold5: preprocessor 1/1\n\n\ni Fold5: preprocessor 1/1, model 1/25\n\n\ni Fold5: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 2/25\n\n\ni Fold5: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 3/25\n\n\ni Fold5: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 4/25\n\n\ni Fold5: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 5/25\n\n\ni Fold5: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 6/25\n\n\ni Fold5: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 7/25\n\n\ni Fold5: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 8/25\n\n\ni Fold5: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 9/25\n\n\ni Fold5: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 10/25\n\n\ni Fold5: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 11/25\n\n\ni Fold5: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 12/25\n\n\ni Fold5: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 13/25\n\n\ni Fold5: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 14/25\n\n\ni Fold5: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 15/25\n\n\ni Fold5: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 16/25\n\n\ni Fold5: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 17/25\n\n\ni Fold5: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 18/25\n\n\ni Fold5: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 19/25\n\n\ni Fold5: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 20/25\n\n\ni Fold5: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 21/25\n\n\ni Fold5: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 22/25\n\n\ni Fold5: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 23/25\n\n\ni Fold5: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 24/25\n\n\ni Fold5: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 25/25\n\n\ni Fold5: preprocessor 1/1, model 25/25 (predictions)\n\n# Stop cluster when done\nstopCluster(cl)\nregisterDoSEQ()\n\n\n# collect metrics\ntree_metrics &lt;- collect_metrics(tree_res)\nhead(tree_metrics)\n\n# A tibble: 6 × 8\n  cost_complexity tree_depth .metric     .estimator  mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1         0.00001          2 mn_log_loss binary     0.404     5  9.73e-6 pre0_m…\n2         0.00001          4 mn_log_loss binary     0.358     5  3.91e-4 pre0_m…\n3         0.00001          6 mn_log_loss binary     0.350     5  1.01e-3 pre0_m…\n4         0.00001          8 mn_log_loss binary     0.341     5  2.18e-3 pre0_m…\n5         0.00001         10 mn_log_loss binary     0.384     5  2.03e-3 pre0_m…\n6         0.0001           2 mn_log_loss binary     0.404     5  9.73e-6 pre0_m…\n\n\n\n\nSelect the best model\n\nbest_tree &lt;- select_best(tree_res, metric = \"mn_log_loss\")\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1         0.00001          8 pre0_mod04_post0\n\n\n\n\nFinal tree workflow\n\nfinal_tree_wf &lt;- finalize_workflow(tree_wf, best_tree)\nfinal_tree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-05\n  tree_depth = 8\n  min_n = 5\n\nComputational engine: rpart \n\n\n\n\nPart B: Random Forest\n\nRandom forest specification (with tuning)\nWe’ll tune: mtry (number of variables sampled at each split) min_n (minimum observations required at a node)\n\nrf_model &lt;- rand_forest(\n  mtry = tune(),\n  min_n = tune(),\n  trees = 100\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\nRandom Forest Grid\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, 10)),\n  min_n(range = c(5, 50)),\n  levels = 5\n)\n\n\n\nRandom Forest Workflow\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(diabetes_recipe)\n\n\n\nTune with LOG-LOSS\n\ncl   &lt;- makeCluster(parallel::detectCores() - 1)\nregisterDoParallel(cl)\n\nctrl &lt;- control_grid(\n  save_pred     = FALSE,\n  save_workflow = FALSE,\n  verbose       = TRUE,\n  allow_par     = TRUE\n)\n\n\nset.seed(129)\nrf_res &lt;- tune_grid(\n  rf_wf,\n  resamples = cv_folds,\n  metrics   = metric_set(mn_log_loss),\n  grid      = rf_grid,\n  control   = ctrl\n)\n\ni Fold1: preprocessor 1/1\n\n\ni Fold1: preprocessor 1/1, model 1/25\n\n\ni Fold1: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 2/25\n\n\ni Fold1: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 3/25\n\n\ni Fold1: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 4/25\n\n\ni Fold1: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 5/25\n\n\ni Fold1: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 6/25\n\n\ni Fold1: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 7/25\n\n\ni Fold1: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 8/25\n\n\ni Fold1: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 9/25\n\n\ni Fold1: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 10/25\n\n\ni Fold1: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 11/25\n\n\ni Fold1: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 12/25\n\n\ni Fold1: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 13/25\n\n\ni Fold1: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 14/25\n\n\ni Fold1: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 15/25\n\n\ni Fold1: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 16/25\n\n\ni Fold1: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 17/25\n\n\ni Fold1: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 18/25\n\n\ni Fold1: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 19/25\n\n\ni Fold1: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 20/25\n\n\ni Fold1: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 21/25\n\n\ni Fold1: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 22/25\n\n\ni Fold1: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 23/25\n\n\ni Fold1: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 24/25\n\n\ni Fold1: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold1: preprocessor 1/1, model 25/25\n\n\ni Fold1: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold2: preprocessor 1/1\n\n\ni Fold2: preprocessor 1/1, model 1/25\n\n\ni Fold2: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 2/25\n\n\ni Fold2: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 3/25\n\n\ni Fold2: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 4/25\n\n\ni Fold2: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 5/25\n\n\ni Fold2: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 6/25\n\n\ni Fold2: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 7/25\n\n\ni Fold2: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 8/25\n\n\ni Fold2: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 9/25\n\n\ni Fold2: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 10/25\n\n\ni Fold2: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 11/25\n\n\ni Fold2: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 12/25\n\n\ni Fold2: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 13/25\n\n\ni Fold2: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 14/25\n\n\ni Fold2: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 15/25\n\n\ni Fold2: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 16/25\n\n\ni Fold2: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 17/25\n\n\ni Fold2: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 18/25\n\n\ni Fold2: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 19/25\n\n\ni Fold2: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 20/25\n\n\ni Fold2: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 21/25\n\n\ni Fold2: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 22/25\n\n\ni Fold2: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 23/25\n\n\ni Fold2: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 24/25\n\n\ni Fold2: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold2: preprocessor 1/1, model 25/25\n\n\ni Fold2: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold3: preprocessor 1/1\n\n\ni Fold3: preprocessor 1/1, model 1/25\n\n\ni Fold3: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 2/25\n\n\ni Fold3: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 3/25\n\n\ni Fold3: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 4/25\n\n\ni Fold3: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 5/25\n\n\ni Fold3: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 6/25\n\n\ni Fold3: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 7/25\n\n\ni Fold3: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 8/25\n\n\ni Fold3: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 9/25\n\n\ni Fold3: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 10/25\n\n\ni Fold3: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 11/25\n\n\ni Fold3: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 12/25\n\n\ni Fold3: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 13/25\n\n\ni Fold3: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 14/25\n\n\ni Fold3: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 15/25\n\n\ni Fold3: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 16/25\n\n\ni Fold3: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 17/25\n\n\ni Fold3: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 18/25\n\n\ni Fold3: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 19/25\n\n\ni Fold3: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 20/25\n\n\ni Fold3: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 21/25\n\n\ni Fold3: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 22/25\n\n\ni Fold3: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 23/25\n\n\ni Fold3: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 24/25\n\n\ni Fold3: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold3: preprocessor 1/1, model 25/25\n\n\ni Fold3: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold4: preprocessor 1/1\n\n\ni Fold4: preprocessor 1/1, model 1/25\n\n\ni Fold4: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 2/25\n\n\ni Fold4: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 3/25\n\n\ni Fold4: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 4/25\n\n\ni Fold4: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 5/25\n\n\ni Fold4: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 6/25\n\n\ni Fold4: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 7/25\n\n\ni Fold4: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 8/25\n\n\ni Fold4: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 9/25\n\n\ni Fold4: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 10/25\n\n\ni Fold4: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 11/25\n\n\ni Fold4: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 12/25\n\n\ni Fold4: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 13/25\n\n\ni Fold4: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 14/25\n\n\ni Fold4: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 15/25\n\n\ni Fold4: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 16/25\n\n\ni Fold4: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 17/25\n\n\ni Fold4: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 18/25\n\n\ni Fold4: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 19/25\n\n\ni Fold4: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 20/25\n\n\ni Fold4: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 21/25\n\n\ni Fold4: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 22/25\n\n\ni Fold4: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 23/25\n\n\ni Fold4: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 24/25\n\n\ni Fold4: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold4: preprocessor 1/1, model 25/25\n\n\ni Fold4: preprocessor 1/1, model 25/25 (predictions)\n\n\ni Fold5: preprocessor 1/1\n\n\ni Fold5: preprocessor 1/1, model 1/25\n\n\ni Fold5: preprocessor 1/1, model 1/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 2/25\n\n\ni Fold5: preprocessor 1/1, model 2/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 3/25\n\n\ni Fold5: preprocessor 1/1, model 3/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 4/25\n\n\ni Fold5: preprocessor 1/1, model 4/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 5/25\n\n\ni Fold5: preprocessor 1/1, model 5/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 6/25\n\n\ni Fold5: preprocessor 1/1, model 6/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 7/25\n\n\ni Fold5: preprocessor 1/1, model 7/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 8/25\n\n\ni Fold5: preprocessor 1/1, model 8/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 9/25\n\n\ni Fold5: preprocessor 1/1, model 9/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 10/25\n\n\ni Fold5: preprocessor 1/1, model 10/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 11/25\n\n\ni Fold5: preprocessor 1/1, model 11/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 12/25\n\n\ni Fold5: preprocessor 1/1, model 12/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 13/25\n\n\ni Fold5: preprocessor 1/1, model 13/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 14/25\n\n\ni Fold5: preprocessor 1/1, model 14/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 15/25\n\n\ni Fold5: preprocessor 1/1, model 15/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 16/25\n\n\ni Fold5: preprocessor 1/1, model 16/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 17/25\n\n\ni Fold5: preprocessor 1/1, model 17/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 18/25\n\n\ni Fold5: preprocessor 1/1, model 18/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 19/25\n\n\ni Fold5: preprocessor 1/1, model 19/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 20/25\n\n\ni Fold5: preprocessor 1/1, model 20/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 21/25\n\n\ni Fold5: preprocessor 1/1, model 21/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 22/25\n\n\ni Fold5: preprocessor 1/1, model 22/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 23/25\n\n\ni Fold5: preprocessor 1/1, model 23/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 24/25\n\n\ni Fold5: preprocessor 1/1, model 24/25 (predictions)\n\n\ni Fold5: preprocessor 1/1, model 25/25\n\n\ni Fold5: preprocessor 1/1, model 25/25 (predictions)\n\nstopCluster(cl)\nregisterDoSEQ()\n\n# collect metrics\nref_metrics &lt;- collect_metrics(rf_res)\nhead(ref_metrics)\n\n# A tibble: 6 × 8\n   mtry min_n .metric     .estimator  mean     n  std_err .config         \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;           \n1     2     5 mn_log_loss binary     0.329     5 0.000608 pre0_mod01_post0\n2     2    16 mn_log_loss binary     0.330     5 0.000784 pre0_mod02_post0\n3     2    27 mn_log_loss binary     0.330     5 0.000391 pre0_mod03_post0\n4     2    38 mn_log_loss binary     0.329     5 0.000474 pre0_mod04_post0\n5     2    50 mn_log_loss binary     0.330     5 0.000668 pre0_mod05_post0\n6     4     5 mn_log_loss binary     0.320     5 0.000652 pre0_mod06_post0\n\n\n\n\nSelect best random forest\n\nbest_rf &lt;- select_best(rf_res, metric = \"mn_log_loss\")\nbest_rf\n\n# A tibble: 1 × 3\n   mtry min_n .config         \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1     6    50 pre0_mod15_post0\n\n\n\n\nFinal random forest workflow\n\nfinal_rf_wf &lt;- finalize_workflow(rf_wf, best_rf)\n\n\n\nEvaluate models on the test data\n\n\nClassification Tree\n\ntree_fit &lt;- final_tree_wf %&gt;% fit(data = train_data)\ntree_pred &lt;- predict(tree_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(Diabetes_binary))\nnames(tree_pred)\n\n[1] \".pred_No diabetes\" \".pred_Diabetes\"    \"Diabetes_binary\"  \n\nmn_log_loss(tree_pred, truth = Diabetes_binary, .pred_Diabetes)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary          2.32\n\n\n\n\nRandom Forest\n\nrf_fit &lt;- final_rf_wf %&gt;% fit(data = train_data)\nrf_pred &lt;- predict(rf_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(Diabetes_binary))\n\nmn_log_loss(rf_pred, truth = Diabetes_binary, .pred_Diabetes)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary          2.54\n\n\n\n\n\nGenerating Comparison Tables and Plots\n\nComparison Table (Log-Loss on Test Set)\n\n# Compute log-loss for each model\ntree_logloss &lt;- mn_log_loss(tree_pred, truth = Diabetes_binary, .pred_Diabetes)\nrf_logloss   &lt;- mn_log_loss(rf_pred, truth = Diabetes_binary, .pred_Diabetes)\n\ncomparison_table &lt;- tibble(\n  Model = c(\"Decision Tree\", \"Random Forest\"),\n  LogLoss = c(tree_logloss$.estimate, rf_logloss$.estimate)\n)\n\ncomparison_table\n\n# A tibble: 2 × 2\n  Model         LogLoss\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Decision Tree    2.32\n2 Random Forest    2.54\n\n\n\n\nPlot Tuning Results for Each Model\n\nTree tuning plot\nShows how changing depth & cost-complexity influenced log-loss.\n\nautoplot(tree_res) +\n  ggtitle(\"Classification Tree – Log-Loss Across Tuning Grid\")\n\n\n\n\n\n\n\n\n\n\nRandom Forest tuning plot\nShows effect of different mtry and min_n.\n\nautoplot(rf_res) +\n  ggtitle(\"Random Forest – Log-Loss Across Tuning Grid\")\n\n\n\n\n\n\n\n\n\n\nSide-by-side comparison plot (best vs best)\n\ncomparison_table %&gt;%\n  ggplot(aes(x = Model, y = LogLoss, fill = Model)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(LogLoss, 3)), vjust = -0.2, size = 5) +\n  labs(\n    title = \"Model Comparison: Log-Loss on Test Set\",\n    y = \"Log-Loss\",\n    x = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Modeling.html#task3-cross-validation-set-up-5-fold-stratified",
    "href": "Modeling.html#task3-cross-validation-set-up-5-fold-stratified",
    "title": "Predictive Modeling",
    "section": "Task3: Cross-validation set up (5-fold, stratified)",
    "text": "Task3: Cross-validation set up (5-fold, stratified)\nSplit into 5 folds, Each fold is used as a validation set once while the remaining 4 folds are used for training.\n\nset.seed(124)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = Diabetes_binary)\ncv_folds\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [142059/35516]&gt; Fold1\n2 &lt;split [142059/35516]&gt; Fold2\n3 &lt;split [142060/35515]&gt; Fold3\n4 &lt;split [142061/35514]&gt; Fold4\n5 &lt;split [142061/35514]&gt; Fold5\n\n\n##Task 4 Recipe: Basic recipe using all predictors.\n\n# since classification Tree and random forest models can take on factor variables, we didn't do the preprocessing for creating dummy variables.here, selected the predictors for the model.\ndiabetes_recipe &lt;- recipe(Diabetes_binary ~ HighBP + BMI + PhysActivity + Age_group + \n                GenHlth + Smoker + Education, data = train_data) %&gt;%\n  step_zv(all_predictors()) #to remove predictors with zero variance sd                                 they are not important in the modeling\ndiabetes_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 7\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "Modeling.html#task4",
    "href": "Modeling.html#task4",
    "title": "Predictive Modeling",
    "section": "Task4:",
    "text": "Task4:\n\nA.Classification Tree\nClassification Tree model is a type of supervised learning algorithm that is used to predict categorical outcome. So in our data, predicting whether participant has diabetes or not.It works by repeatedly splitting the data into smaller, more homogeneous groups.At each step, the algorithm selects the variable and cutoff that best separate the outcome categories—typically using metrics like Gini impurity or classification error. The result is a tree-like structure where each internal node represents a decision rule, each branch represents an outcome of that decision, and each terminal leaf represents a predicted class. Even though they are easy to interpret, single trees can be unstable, which means small changes in the data can produce different split.\n\nModel specification (with parameters to tune)\nWe’ll use tree depth and cost-complexity (cp) to optimize the model’s performance .\n\ntree_model &lt;- decision_tree(\n  cost_complexity = tune(),#pruning parameter\n  tree_depth = tune(),#how many levels from the root node to the final node\n  min_n = 5  # minimum number of samples to split a node\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\ntree_model\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 5\n\nComputational engine: rpart \n\n\n\n\nGrid of tuning parameters ( custom grid)\ncreate grid of possible values for the model on how much to prune and how deep to let it grow. we’re providing R with Ranges are the smallest and largest values\n\ntree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-5, -1)), \n  tree_depth(range = c(2, 10)),\n  levels = 5\n)\ntree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1         0.00001          2\n 2         0.0001           2\n 3         0.001            2\n 4         0.01             2\n 5         0.1              2\n 6         0.00001          4\n 7         0.0001           4\n 8         0.001            4\n 9         0.01             4\n10         0.1              4\n# ℹ 15 more rows\n\n\n\n\nWorkflow for the tree\nBuild a workflow to combine the pre-processed data with the model\n\ntree_wf &lt;- workflow() %&gt;%\n  add_model(tree_model) %&gt;%\n  add_recipe(diabetes_recipe)\ntree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 5\n\nComputational engine: rpart \n\n\n\n\nTuning using LOG-LOSS\nHere, we are trying all the model settings in the grid created, tests each one using cv, and finds the combination that gives the best performance.\n\n# Set up parallel backend \ncl &lt;- makeCluster(parallel::detectCores() - 1)\nregisterDoParallel(cl)\n\n# Control settings for tuning\nctrl &lt;- control_grid(\n  save_pred     = FALSE,\n  save_workflow = FALSE,\n  verbose       = FALSE,\n  allow_par     = TRUE\n)\n\n# Run tuning\nset.seed(112)\ntree_res &lt;- tune_grid(\n  tree_wf,\n  resamples = cv_folds,\n  metrics   = metric_set(mn_log_loss),\n  grid      = tree_grid,   \n  control   = ctrl\n)\n\n# Stop cluster when done\nstopCluster(cl)\nregisterDoSEQ()\n\n\n# collect metrics\ntree_metrics &lt;- collect_metrics(tree_res)\nhead(tree_metrics)\n\n# A tibble: 6 × 8\n  cost_complexity tree_depth .metric     .estimator  mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1         0.00001          2 mn_log_loss binary     0.404     5  9.73e-6 pre0_m…\n2         0.00001          4 mn_log_loss binary     0.364     5  1.01e-2 pre0_m…\n3         0.00001          6 mn_log_loss binary     0.352     5  1.37e-3 pre0_m…\n4         0.00001          8 mn_log_loss binary     0.337     5  9.32e-4 pre0_m…\n5         0.00001         10 mn_log_loss binary     0.343     5  2.48e-3 pre0_m…\n6         0.0001           2 mn_log_loss binary     0.404     5  9.73e-6 pre0_m…\n\n\n\n\nSelect the best model\nthis picks the best performing model from the tuning outputs.\n\nbest_tree &lt;- select_best(tree_res, metric = \"mn_log_loss\")\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1         0.00001          8 pre0_mod04_post0\n\n\n\n\nFinal tree workflow\nHere we’re updating the work flow with the best performing hyperparameter values.\n\nfinal_tree_wf &lt;- finalize_workflow(tree_wf, best_tree)\nfinal_tree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-05\n  tree_depth = 8\n  min_n = 5\n\nComputational engine: rpart \n\n\n\n\n\nB. Random Forest\nA random forest is an ensemble method that builds many decision trees and combines their predictions to produce a more accurate and stable result. Each tree is trained on a bootstrapped sample, and at each split, the algorithm considers only a random subset of predictors. This randomness makes the trees more diverse, which helps the forest capture different patterns in the data.It also helps to decorrelates the trees which in turn reduce variance and improve accuracy. Random forests are powerful because they reduce the common weaknesses of single classification trees—especially overfitting and instability. By averaging or voting across many trees, the model becomes more robust and performs better on new, unseen data.\n\nRandom forest specification (with tuning)\nWe’ll tune on: mtry (number of predictor variables sampled at each split) min_n (minimum observations required at a node)\n\nrf_model &lt;- rand_forest(\n  mtry = tune(),\n  min_n = tune(),\n  trees = 100 \n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\nRandom Forest Grid\nThis creates 25 combination of mtry and min_n values for the model to test to find the best setting.\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, 7)),\n  min_n(range = c(5, 50)),\n  levels = 5\n)\n\n\n\nRandom Forest Workflow\nBuild a workflow to combine the pre-processed data with the model\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(diabetes_recipe)\n\n\n\nTune with LOG-LOSS\nHere, we are trying all the model settings in the grid created, tests each one using cv, and finds the combination that gives the best performance.\n\ncl   &lt;- makeCluster(parallel::detectCores() - 1)\nregisterDoParallel(cl)\n\nctrl &lt;- control_grid(\n  save_pred     = FALSE,\n  save_workflow = FALSE,\n  verbose       = FALSE,\n  allow_par     = TRUE\n)\n\n\nset.seed(129)\nrf_res &lt;- tune_grid(\n  rf_wf,\n  resamples = cv_folds,\n  metrics   = metric_set(mn_log_loss),\n  grid      = rf_grid,\n  control   = ctrl\n)\n\nstopCluster(cl)\nregisterDoSEQ()\n\n# collect metrics\nref_metrics &lt;- collect_metrics(rf_res)\nhead(ref_metrics)\n\n# A tibble: 6 × 8\n   mtry min_n .metric     .estimator  mean     n  std_err .config         \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;           \n1     2     5 mn_log_loss binary     0.323     5 0.000475 pre0_mod01_post0\n2     2    16 mn_log_loss binary     0.323     5 0.000467 pre0_mod02_post0\n3     2    27 mn_log_loss binary     0.323     5 0.000537 pre0_mod03_post0\n4     2    38 mn_log_loss binary     0.323     5 0.000458 pre0_mod04_post0\n5     2    50 mn_log_loss binary     0.323     5 0.000532 pre0_mod05_post0\n6     3     5 mn_log_loss binary     0.331     5 0.000400 pre0_mod06_post0\n\n\n\n\nSelect best random forest\n\nbest_rf &lt;- select_best(rf_res, metric = \"mn_log_loss\")\nbest_rf\n\n# A tibble: 1 × 3\n   mtry min_n .config         \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1     2    50 pre0_mod05_post0\n\n\n\n\nFinal random forest workflow\n\nfinal_rf_wf &lt;- finalize_workflow(rf_wf, best_rf)\n\n\n\n\nC.Evaluate models on the test data\n\nClassification Tree\nhere, we’re fitting the final classification tree model on the training dataset, predict probability of diabetes on the test data and calculate the log loss.\n\ntree_fit &lt;- final_tree_wf %&gt;% fit(data = train_data)\ntree_pred &lt;- predict(tree_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(Diabetes_binary))\nnames(tree_pred)\n\n[1] \".pred_No diabetes\" \".pred_Diabetes\"    \"Diabetes_binary\"  \n\nmn_log_loss(tree_pred, truth = Diabetes_binary, .pred_Diabetes)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary          2.29\n\n\n\n\nRandom Forest\nhere, we’re fitting the final random forest model on the training dataset, predict probability of diabetes on the test data and calculate the log loss.\n\nrf_fit &lt;- final_rf_wf %&gt;% fit(data = train_data)\nrf_pred &lt;- predict(rf_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(Diabetes_binary))\n\nmn_log_loss(rf_pred, truth = Diabetes_binary, .pred_Diabetes)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary          2.41\n\n\n\n\n\nD. Generating Comparison Tables and Plots\n\nComparison Table (Log-Loss on Test Set)\n\n# Compute log-loss for each model\ntree_logloss &lt;- mn_log_loss(tree_pred, truth = Diabetes_binary, .pred_Diabetes)\nrf_logloss   &lt;- mn_log_loss(rf_pred, truth = Diabetes_binary, .pred_Diabetes)\n\ncomparison_table &lt;- tibble(\n  Model = c(\"Decision Tree\", \"Random Forest\"),\n  LogLoss = c(tree_logloss$.estimate, rf_logloss$.estimate)\n)\n\ncomparison_table\n\n# A tibble: 2 × 2\n  Model         LogLoss\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Decision Tree    2.29\n2 Random Forest    2.41\n\n\n\n\nPlot Tuning Results for Each Model\n\nTree tuning plot\nShows how changing depth & cost-complexity influenced log-loss.\n\nautoplot(tree_res) +\n  ggtitle(\"Classification Tree – Log-Loss Across Tuning Grid\")\n\n\n\n\n\n\n\n\n\n\nRandom Forest tuning plot\nShows effect of different mtry and min_n.\n\nautoplot(rf_res) +\n  ggtitle(\"Random Forest – Log-Loss Across Tuning Grid\")\n\n\n\n\n\n\n\n\n\n\nSide-by-side comparison plot (best vs best)\n\ncomparison_table %&gt;%\n  ggplot(aes(x = Model, y = LogLoss, fill = Model)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = round(LogLoss, 3)), vjust = -0.2, size = 5) +\n  labs(\n    title = \"Model Comparison: Log-Loss on Test Set\",\n    y = \"Log-Loss\",\n    x = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nBased on the log-loss result above, the classification tree performed better than the random forest on the test set. The test-set log-loss values were approximately 2.3 for the decision tree and 2.4 for the random forest, indicating that the tree produced better-tuned probability predictions for predicting diabetes in the dataset."
  }
]